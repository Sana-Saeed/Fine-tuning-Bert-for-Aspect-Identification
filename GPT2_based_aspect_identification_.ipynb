{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOvA2k8nfdAo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load XLNet tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "# Load the dataset from CSV\n",
        "df = pd.read_csv('reviews.csv')  # Ensure the CSV has 'Sentence' and 'Aspects' columns\n",
        "\n",
        "# Extract columns\n",
        "sentences = df['Sentence'].tolist()\n",
        "aspects = df['Aspects'].tolist()\n",
        "\n",
        "# Assign binary labels (e.g., 0 for negative, 1 for positive). Adjust this as per your dataset's labeling.\n",
        "labels = [1 if \"good\" in sentence.lower() else 0 for sentence in sentences]\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess(sentences, aspects, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    label_list = []\n",
        "\n",
        "    for sentence, aspect, label in zip(sentences, aspects, labels):\n",
        "        encoded = tokenizer(\n",
        "            f\"{sentence} [SEP] {aspect}\",\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'].squeeze(0))\n",
        "        attention_masks.append(encoded['attention_mask'].squeeze(0))\n",
        "        label_list.append(label)\n",
        "\n",
        "    return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(label_list)\n",
        "\n",
        "input_ids, attention_masks, labels = preprocess(sentences, aspects, labels)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Load the pre-trained XLNet model\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "total_steps = len(dataloader) * 4  # Assume 4 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch_input_ids, batch_attention_masks, batch_labels = [x.to(device) for x in batch]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation phase (using the same DataLoader for simplicity; ideally, use a separate validation set)\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_predictions, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch_input_ids, batch_attention_masks, batch_labels = [x.to(device) for x in batch]\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            val_predictions.extend(predictions)\n",
        "            val_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(dataloader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix and Metrics\n",
        "conf_matrix = confusion_matrix(val_labels, val_predictions)\n",
        "accuracy = accuracy_score(val_labels, val_predictions)\n",
        "precision = precision_score(val_labels, val_predictions, average='weighted')\n",
        "recall = recall_score(val_labels, val_predictions, average='weighted')\n",
        "f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
        "\n",
        "# F2 Score calculation\n",
        "def fbeta_score(y_true, y_pred, beta):\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
        "\n",
        "f2 = fbeta_score(val_labels, val_predictions, beta=2)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"F2 Score: {f2:.4f}\")\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ]
}